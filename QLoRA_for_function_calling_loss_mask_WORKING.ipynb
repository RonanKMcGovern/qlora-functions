{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RonanKMcGovern/qlora-functions/blob/loss-mask/QLoRA_for_function_calling_loss_mask_WORKING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *About*\n",
        "\n",
        "Forked from: https://colab.research.google.com/drive/1FxlUb_H6Xirhkx4RszAgHeb2uDW7oKIH by https://www.youtube.com/@vrsen\n",
        "\n",
        "Authored by: RonanKMcGovern of https://research.trelis.com and https://huggingface.co/Trelis"
      ],
      "metadata": {
        "id": "dDdy-k2BVDAo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzGa3YSCgl4C"
      },
      "source": [
        "# Install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "oAzlPhWXVKXo",
        "outputId": "339d0e6d-ef2e-4956-bb3b-5cd5830d433e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "cache_dir = \"/content/drive/My Drive/huggingface_cache\"\n",
        "os.makedirs(cache_dir, exist_ok=True) # Ensure the directory exists\n"
      ],
      "metadata": {
        "id": "MH8ZqAEcVLTH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423,
          "referenced_widgets": [
            "2602397d8f07408799c434e24b9e016a",
            "061687b2863f4175acf941bd1583172d",
            "0b0b6450b58c4df49104aed2233574eb",
            "b6b38207915746c9a63a3f71befc11f9",
            "01d43c760716465ea62e32ed97f87ba1",
            "527d8494b2e246568d363708e0df0893",
            "eb2af06ef6364a76bd0b6ce82631befb",
            "4b625a4c3dfa432482b08bdf7fc5c2b0",
            "25dbe2ffe3814d8b97b98166c12c968c",
            "5030173bfc3948f5a0b7f2bbd0173c9e",
            "32a14d4829ab4503aacb5d8900461494",
            "aff724e5b3834e01a9afeb12d4382062",
            "34ff3875e5f440de938de39a048a4668",
            "4732a5ae36a94168bfb08ba150df7299",
            "b69c688451304d788a169045505a5a50",
            "2b214d67aefc49158cf14e0ec4963274",
            "cd93283e1d414bc5981a8bb7e2c23458",
            "aa976b99e42d4a83851f375ba8bfabc8",
            "9d9b135427694f58baa45f0dbdae231e",
            "592b371bfa0442a2a3e104cd0650f0c5",
            "4c10af42b9bf4ff08f0a24cbed43654a",
            "5c3ed6c9fd9a478e9df2e667d4bb9101",
            "2d7a428089924eaaba34112b3a4b6998",
            "3ead846f98af4180988d2e1fdca7a3ef",
            "52aa88b1a2d14c38ad789b1a3c59c727",
            "081c6b5c8ff54e82987fc19b370fce55",
            "cff3f80bb5b347a8818d5108718e07e7",
            "8df459303e944d4195797f9d9ef08c39",
            "e3921eb1a6ae45958a211c3cd08bfeaf",
            "0aee1243c19542019b096099cdcbb70b",
            "2b5e035203a64fc09d8d091881eef5cd",
            "81458769a6ae471f8db82c977d1386e8"
          ]
        },
        "id": "OUXX9i0g-vLs",
        "outputId": "ba20b11f-f84a-4c0a-a780-df3ccba05675"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/268.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m204.8/268.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
            "Installing collected packages: huggingface_hub\n",
            "Successfully installed huggingface_hub-0.16.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2602397d8f07408799c434e24b9e016a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7wgbsa4U2TBk"
      },
      "outputs": [],
      "source": [
        "# https://stackoverflow.com/questions/56081324/why-are-google-colab-shell-commands-not-working\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGeTgx1Hgljb",
        "outputId": "68d7c323-6aae-4e0d-fc0b-ac6bc3ea6ee1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.2/492.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q -U einops\n",
        "!pip install -q -U safetensors\n",
        "!pip install -q -U torch\n",
        "!pip install -q -U xformers\n",
        "!pip install -q -U datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1zodOgcgqnO"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "v_r4SL_KmHGs"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "import transformers\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Quantized Model"
      ],
      "metadata": {
        "id": "alLhGWu9Ngvg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b0622d93960c42b69d01ced14fbe33ac",
            "030e4454827441c5967255edd5aed22f",
            "0acb05d801794e69aac407583417cc33",
            "bd0149d093e8470e80e156e245c0dfcc",
            "ef4b89208f1947a19ef51a91888bb9fb",
            "32459882b922470f8ff1736c9316ad5f",
            "5cab951743a743c1a7e49412da970807",
            "d361c3987d7843b7a7c3648318b1a5cf",
            "50c0efaf9e9b4609bd497170d8d875cd",
            "a7a94b6a4c4e4c49b8d2e271c86c25e7",
            "961e2210685c46da851c951a9cec3077"
          ]
        },
        "id": "tz4eBuxZHi7F",
        "outputId": "169affe9-3b18-45ff-ec2e-6284a0b23fe5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0622d93960c42b69d01ced14fbe33ac"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0}, trust_remote_code=True, cache_dir=cache_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Notes\n",
        "\n",
        "Llama-2-7b-chat-hf (unsharded)\n",
        "Running with High RAM and T4 GPU.\n",
        "\n",
        "CPU is at 4.8 GB\n",
        "GPU is at 6.2 GB"
      ],
      "metadata": {
        "id": "fdnnCdDY2Bcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir)"
      ],
      "metadata": {
        "id": "SEitzH351wCR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"EOS token:\", tokenizer.eos_token)\n",
        "print(\"EOS token id:\", tokenizer.eos_token_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LykWtjrgH4kq",
        "outputId": "72e6c042-d3e5-48d5-c494-13c3d6910f6e"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EOS token: </s>\n",
            "EOS token id: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Pad token: \", tokenizer.pad_token)\n",
        "print(\"Pad token ID: \", tokenizer.pad_token_id)"
      ],
      "metadata": {
        "id": "dzw1jQdD1ZBR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0bf7bce-c9c0-40c4-95aa-e26133e985ff"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using pad_token, but it is not set yet.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pad token:  None\n",
            "Pad token ID:  None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need to set the pad token..."
      ],
      "metadata": {
        "id": "AGdd8TIh1zkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Sample string\n",
        "sample_string = ['hello [/INST]', 'my good friend</s>']\n",
        "\n",
        "# Tokenize the stringified JSON object\n",
        "encoded_sample = tokenizer(sample_string, truncation=True, padding=True, max_length=512, return_tensors='pt', add_special_tokens=True)\n",
        "\n",
        "# Count the number of tokens\n",
        "token_count = len(encoded_sample)\n",
        "\n",
        "# Fetch BOS and EOS tokens\n",
        "BOS_token_id = tokenizer.bos_token_id\n",
        "EOS_token_id = tokenizer.eos_token_id\n",
        "BOS_token = tokenizer.decode([BOS_token_id])\n",
        "EOS_token = tokenizer.decode([EOS_token_id])\n",
        "\n",
        "# Check and print BOS and EOS tokens\n",
        "print(f\"Beginning of the sequence: {sample_string[0]} (BOS token: {BOS_token}, id: {BOS_token_id})\")\n",
        "print(f\"End of the sequence: {sample_string[-1]} (EOS token: {EOS_token}, id: {EOS_token_id})\")\n",
        "\n",
        "print(f\"The number of tokens in the stringified JSON object is: {token_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kByEN730YOh7",
        "outputId": "50b010b2-f71b-420a-ea32-0386be4ebf58"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beginning of the sequence: hello[/INST] (BOS token: <s>, id: 1)\n",
            "End of the sequence: my good friend</s> (EOS token: </s>, id: 2)\n",
            "The number of tokens in the stringified JSON object is: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoded_sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrNuzWVTq-Ny",
        "outputId": "64ef9f8c-27ac-48c2-cd8a-0636e161afba"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[    1, 22172, 29961, 29914, 25580, 29962],\n",
            "        [    2,     1,   590,  1781,  5121,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1],\n",
            "        [0, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Model's maximum length: {tokenizer.model_max_length}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KiINqV0ftJY",
        "outputId": "e7b4e843-f756-48c7-c378-83b500797f16"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's maximum length: 1000000000000000019884624838656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.config)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPYJ5vUNftKm",
        "outputId": "16eb0f94-be10-4cce-b15e-c4a61c7535b0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.32.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up LoRa"
      ],
      "metadata": {
        "id": "iSuBrec41b0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "Hd22_v_mkbZy"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "HOhd6empkd0_"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NPUq-u2ngzw",
        "outputId": "5c78a626-d2c5-49dd-e031-1d9883baf853"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 0 || all params: 3500412928 || trainable%: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.state_dict().keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EBGtk1On8tc",
        "outputId": "3fe47183-3a3f-4121-8e78-51baf4e7a94c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['model.embed_tokens.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.norm.weight', 'lm_head.weight'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UkOrtBBohnL",
        "outputId": "82a7d069-687b-4118-9a98-159fb1e48d75"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 16777216 || all params: 3517190144 || trainable%: 0.477006226934315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Original LoRa config - DO NOT RUN\n",
        "\n",
        "# from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# config = LoraConfig(\n",
        "#     r=16,\n",
        "#     lora_alpha=32,\n",
        "#     target_modules=[\"query_key_value\"],\n",
        "#     lora_dropout=0.05,\n",
        "#     bias=\"none\",\n",
        "#     task_type=\"CAUSAL_LM\"\n",
        "# )\n",
        "\n",
        "# model = get_peft_model(model, config)\n",
        "# print_trainable_parameters(model)"
      ],
      "metadata": {
        "id": "-qjN7qOkkgKI"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Data"
      ],
      "metadata": {
        "id": "mxDuoyQvQfD2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "UZbrRk4J3qVH"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\"Trelis/function_calling\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIVJPMTBAzID",
        "outputId": "92cc9043-bfed-438b-c5b3-7ab118faeb31"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['systemPrompt', 'userPrompt', 'assistantResponse'],\n",
            "        num_rows: 11\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['systemPrompt', 'userPrompt', 'assistantResponse'],\n",
            "        num_rows: 5\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, encodings, response_lengths):\n",
        "        self.encodings = encodings\n",
        "        self.response_lengths = response_lengths\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
        "\n",
        "        # Set labels to be the same as input_ids\n",
        "        item[\"labels\"] = item[\"input_ids\"].clone()\n",
        "\n",
        "        # Shift labels to the left and replace the last position with EOS token\n",
        "        item[\"labels\"][:-1] = item[\"input_ids\"][1:]\n",
        "        item[\"labels\"][-1] = 2  # Replace last position with EOS token ID\n",
        "\n",
        "        # Create a loss mask\n",
        "        response_start_position = item[\"input_ids\"].shape[0] - self.response_lengths[idx]\n",
        "        item[\"loss_mask\"] = torch.zeros_like(item[\"input_ids\"])\n",
        "        item[\"loss_mask\"][response_start_position:] = 1\n",
        "\n",
        "        # print(\"TextDataset getitem outputs:\", item.keys())\n",
        "\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])"
      ],
      "metadata": {
        "id": "nNyFtLkj1G5f"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(dataset, tokenizer):\n",
        "    # Define the roles and markers\n",
        "    B_SYS, E_SYS = \"<<SYS>>\", \"<</SYS>>\"\n",
        "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "\n",
        "    # Create the formatted text with the correct roles for each part of the dialogue\n",
        "    formatted_dataset = dataset.map(\n",
        "        lambda x: {\n",
        "            \"input_text\": \"\".join([\n",
        "                f\"{B_INST} {B_SYS}{x['systemPrompt'].strip()}{E_SYS} \\n\",\n",
        "                f\"{x['userPrompt'].strip()} {E_INST} \\n\",\n",
        "                f\"{x['assistantResponse'].strip()}\",  # appending the EOS token in TextData...\n",
        "            ]),\n",
        "            \"response_text\": \"\".join([\n",
        "                f\"{x['assistantResponse'].strip()}\",  # appending the EOS token in TextData...\n",
        "            ]),\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Tokenize the datasets\n",
        "    encodings = tokenizer([dialogue[\"input_text\"] for dialogue in formatted_dataset], truncation=True, padding=True, max_length=512, return_tensors='pt', add_special_tokens=True)\n",
        "\n",
        "    # Tokenize the response one by one without padding and special tokens for the purpose of calculating length\n",
        "    response_lengths = [len(tokenizer.encode(dialogue[\"response_text\"], truncation=True, max_length=512, add_special_tokens=False)) for dialogue in formatted_dataset]\n",
        "\n",
        "    # Create TextDataset\n",
        "    text_dataset = TextDataset(encodings, response_lengths)\n",
        "\n",
        "    return text_dataset\n"
      ],
      "metadata": {
        "id": "ewsa4W2Z1HfB"
      },
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply function to your datasets\n",
        "train_dataset = prepare_dataset(data['train'], tokenizer)\n",
        "test_dataset = prepare_dataset(data['test'], tokenizer)  # assuming you have a test set named 'data['test']'"
      ],
      "metadata": {
        "id": "WRB550G01J5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "5517b2c59a0640bbb202d6482c50d470",
            "bc9a5d7758b54da3bd51a11d6922ec70",
            "b0b6b97a04354b0fa038055a6eca2ea7",
            "811e8fc7f72848e68808f883f9205407",
            "3a9d90cea97f46a2b1345165a00864f3",
            "d4d3f41ac7d84919a22fa0213a367171",
            "ad35a257b1024b288ef6fc0753188b52",
            "5289454ad9c742febb42229c8cb53cb5",
            "ade44aaa8d0f4c70b71abecc16fe340d",
            "4a3a6bce572042c498f10ff2b53c2b2b",
            "4f64b677d7f14bb6a07ceb5c2760e42d",
            "0f7e11feb6a54f76bc6bac7821610939",
            "f256c5ce29ed4614a3b67538ae177501",
            "830925711ed94f338c00a82fd62bfee2",
            "95708be749314800b9b0f7aef69fb1dd",
            "2614691d48494f53af7c6023b323fb88",
            "a3f8142bd6aa4e069d7c41bbe1f733af",
            "5b5d49f3a731480fb3fc39bdc7cf9ff2",
            "d2ba229b703c473eb5702e4ee91922f5",
            "f58a636fa9b4439bb8bb7db719e146a1",
            "a2ff1d56073f4cd78c8b137ba8623b11",
            "009a64ceb3fa4f8b99f34bd1fef908a8"
          ]
        },
        "outputId": "800c3c80-a20f-46af-e8a9-a7dcd9f244f5"
      },
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/11 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5517b2c59a0640bbb202d6482c50d470"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f7e11feb6a54f76bc6bac7821610939"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Examine the datasets"
      ],
      "metadata": {
        "id": "NLmXcTTV2RdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the number of items in the dataset\n",
        "print(f\"Number of samples in the dataset: {len(train_dataset)}\")\n",
        "\n",
        "# Get a sample item\n",
        "sample_item = train_dataset[0]  # replace 0 with the index of the sample you want to examine\n",
        "\n",
        "# Print the dimensions of the sample item\n",
        "print(f\"Dimensions of input_ids: {sample_item['input_ids'].shape}\")\n",
        "print(f\"Dimensions of attention_mask: {sample_item['attention_mask'].shape}\")\n",
        "print(f\"Dimensions of loss_mask: {sample_item['loss_mask'].shape}\")\n",
        "print(f\"Dimensions of labels: {sample_item['labels'].shape}\")\n",
        "\n",
        "# Print some tokens from the start and end of the sample\n",
        "num_tokens_to_print = 38  # replace with the number of tokens you want to print\n",
        "\n",
        "print(\"\\nTokens at the start of the sample:\")\n",
        "print(sample_item['input_ids'][:num_tokens_to_print].tolist())\n",
        "print(tokenizer.convert_ids_to_tokens(sample_item['input_ids'][:num_tokens_to_print].tolist()))\n",
        "\n",
        "print(\"\\nLabels at the start of the sample:\")\n",
        "print(sample_item['labels'][:num_tokens_to_print].tolist())\n",
        "print(tokenizer.convert_ids_to_tokens(sample_item['labels'][:num_tokens_to_print].tolist()))\n",
        "\n",
        "print(\"Attention mask at the start of the sample:\")\n",
        "print(sample_item['attention_mask'][:num_tokens_to_print].tolist())\n",
        "\n",
        "print(\"Loss mask at the start of the sample:\")\n",
        "print(sample_item['loss_mask'][:num_tokens_to_print].tolist())\n",
        "\n",
        "print(\"\\nTokens at the end of the sample:\")\n",
        "print(sample_item['input_ids'][-num_tokens_to_print:].tolist())\n",
        "print(tokenizer.convert_ids_to_tokens(sample_item['input_ids'][-num_tokens_to_print:].tolist()))\n",
        "\n",
        "print(\"\\nLabels at the end of the sample:\")\n",
        "print(sample_item['labels'][-num_tokens_to_print:].tolist())\n",
        "print(tokenizer.convert_ids_to_tokens(sample_item['labels'][-num_tokens_to_print:].tolist()))\n",
        "\n",
        "print(\"Attention mask at the end of the sample:\")\n",
        "print(sample_item['attention_mask'][-num_tokens_to_print:].tolist())\n",
        "\n",
        "print(\"Loss mask at the end of the sample:\")\n",
        "print(sample_item['loss_mask'][-num_tokens_to_print:].tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaR9H_pH2fOg",
        "outputId": "7062717e-4dca-4b22-c85d-2775425316a4"
      },
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples in the dataset: 11\n",
            "Dimensions of input_ids: torch.Size([462])\n",
            "Dimensions of attention_mask: torch.Size([462])\n",
            "Dimensions of loss_mask: torch.Size([462])\n",
            "Dimensions of labels: torch.Size([462])\n",
            "\n",
            "Tokens at the start of the sample:\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 518, 25580, 29962, 3532, 14816, 29903, 6778, 3492, 526, 263, 8444, 5925, 20255, 29889, 450, 1494, 3168]\n",
            "['</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '<s>', '▁[', 'INST', ']', '▁<<', 'SY', 'S', '>>', 'You', '▁are', '▁a', '▁helpful', '▁research', '▁assistant', '.', '▁The', '▁following', '▁functions']\n",
            "\n",
            "Labels at the start of the sample:\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 518, 25580, 29962, 3532, 14816, 29903, 6778, 3492, 526, 263, 8444, 5925, 20255, 29889, 450, 1494, 3168, 29892]\n",
            "['</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '<s>', '▁[', 'INST', ']', '▁<<', 'SY', 'S', '>>', 'You', '▁are', '▁a', '▁helpful', '▁research', '▁assistant', '.', '▁The', '▁following', '▁functions', ',']\n",
            "Attention mask at the start of the sample:\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Loss mask at the start of the sample:\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "\n",
            "Tokens at the end of the sample:\n",
            "[29914, 25580, 29962, 29871, 13, 741, 29908, 12154, 16203, 5931, 2220, 29918, 4804, 23370, 5931, 978, 16203, 5931, 4478, 29918, 10549, 23370, 5931, 25699, 16203, 741, 29908, 1972, 16203, 5931, 381, 728, 1886, 29893, 9522, 5547, 5931, 930]\n",
            "['/', 'INST', ']', '▁', '<0x0A>', '{\\\\', '\"', 'role', '\\\\\":', '\\\\\"', 'function', '_', 'call', '\\\\\",', '\\\\\"', 'name', '\\\\\":', '\\\\\"', 'search', '_', 'bing', '\\\\\",', '\\\\\"', 'arguments', '\\\\\":', '{\\\\', '\"', 'query', '\\\\\":', '\\\\\"', 'ir', 'ish', '▁ste', 'w', '▁reci', 'pes', '\\\\\"', '}}']\n",
            "\n",
            "Labels at the end of the sample:\n",
            "[25580, 29962, 29871, 13, 741, 29908, 12154, 16203, 5931, 2220, 29918, 4804, 23370, 5931, 978, 16203, 5931, 4478, 29918, 10549, 23370, 5931, 25699, 16203, 741, 29908, 1972, 16203, 5931, 381, 728, 1886, 29893, 9522, 5547, 5931, 930, 2]\n",
            "['INST', ']', '▁', '<0x0A>', '{\\\\', '\"', 'role', '\\\\\":', '\\\\\"', 'function', '_', 'call', '\\\\\",', '\\\\\"', 'name', '\\\\\":', '\\\\\"', 'search', '_', 'bing', '\\\\\",', '\\\\\"', 'arguments', '\\\\\":', '{\\\\', '\"', 'query', '\\\\\":', '\\\\\"', 'ir', 'ish', '▁ste', 'w', '▁reci', 'pes', '\\\\\"', '}}', '</s>']\n",
            "Attention mask at the end of the sample:\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Loss mask at the end of the sample:\n",
            "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to print some samples\n",
        "def print_samples(dataset, tokenizer, num_samples=5):\n",
        "    for i, sample in enumerate(dataset):\n",
        "        if i >= num_samples:  # Stop after printing the desired number of samples\n",
        "            break\n",
        "        input_ids = sample['input_ids']\n",
        "        attention_mask = sample['attention_mask']\n",
        "        labels = sample['labels']\n",
        "\n",
        "        # Decode the input and the label\n",
        "        decoded_input = tokenizer.decode(input_ids, skip_special_tokens=False)\n",
        "        decoded_label = tokenizer.decode(labels, skip_special_tokens=False)\n",
        "\n",
        "        print(f\"Sample {i+1}:\")\n",
        "        print(f\"Input: {decoded_input}\")\n",
        "        print(f\"Label: {decoded_label}\")\n",
        "        print(\"----------------------\")\n",
        "\n",
        "# Use the function to print some samples from your train_dataset\n",
        "print_samples(train_dataset, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty8yD1UE79qg",
        "outputId": "076a35cc-5186-4255-a173-16113395f7d1"
      },
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 1:\n",
            "Input: </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s> [INST] <<SYS>>You are a helpful research assistant. The following functions, if any, are available for you to fetch further data to answer user questions: {\\\"name\\\":\\\"search_bing\\\",\\\"description\\\":\\\"Search the web for content on Bing. This allows users to search online/the internet/the web for content.\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"query\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The search query string\\\"}},\\\"required\\\":[\\\"query\\\"]}}{\\\"name\\\":\\\"get_current_weather\\\",\\\"description\\\":\\\"Get the current weather in a given location\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"location\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The city and state, e.g. San Francisco, CA\\\"},\\\"unit\\\":{\\\"type\\\":\\\"string\\\",\\\"enum\\\":[\\\"celsius\\\",\\\"fahrenheit\\\"]}},\\\"required\\\":[\\\"location\\\"]}}{\\\"name\\\":\\\"search_arxiv\\\",\\\"description\\\":\\\"Search for research papers on ArXiv. Make use of AND, OR and NOT operators as appropriate to join terms within the query.\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"query\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The search query string\\\"}},\\\"required\\\":[\\\"query\\\"]}} To make a function call, respond with only a stringified json object containing the function_call role, the name of the function, and the function arguments: {\\\"role\\\":\\\"function_call\\\",\\\"name\\\":\\\"function_name\\\",\\\"arguments\\\":{\\\"argument_name_1\\\":\\\"argument_value_1\\\",\\\"argument_name_2\\\":\\\"argument_value_2\\\"}}<</SYS>> \n",
            "Search the internet for Irish stew recipes. [/INST] \n",
            "{\\\"role\\\":\\\"function_call\\\",\\\"name\\\":\\\"search_bing\\\",\\\"arguments\\\":{\\\"query\\\":\\\"irish stew recipes\\\"}}\n",
            "Label: </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s> [INST] <<SYS>>You are a helpful research assistant. The following functions, if any, are available for you to fetch further data to answer user questions: {\\\"name\\\":\\\"search_bing\\\",\\\"description\\\":\\\"Search the web for content on Bing. This allows users to search online/the internet/the web for content.\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"query\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The search query string\\\"}},\\\"required\\\":[\\\"query\\\"]}}{\\\"name\\\":\\\"get_current_weather\\\",\\\"description\\\":\\\"Get the current weather in a given location\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"location\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The city and state, e.g. San Francisco, CA\\\"},\\\"unit\\\":{\\\"type\\\":\\\"string\\\",\\\"enum\\\":[\\\"celsius\\\",\\\"fahrenheit\\\"]}},\\\"required\\\":[\\\"location\\\"]}}{\\\"name\\\":\\\"search_arxiv\\\",\\\"description\\\":\\\"Search for research papers on ArXiv. Make use of AND, OR and NOT operators as appropriate to join terms within the query.\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"query\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The search query string\\\"}},\\\"required\\\":[\\\"query\\\"]}} To make a function call, respond with only a stringified json object containing the function_call role, the name of the function, and the function arguments: {\\\"role\\\":\\\"function_call\\\",\\\"name\\\":\\\"function_name\\\",\\\"arguments\\\":{\\\"argument_name_1\\\":\\\"argument_value_1\\\",\\\"argument_name_2\\\":\\\"argument_value_2\\\"}}<</SYS>> \n",
            "Search the internet for Irish stew recipes. [/INST] \n",
            "{\\\"role\\\":\\\"function_call\\\",\\\"name\\\":\\\"search_bing\\\",\\\"arguments\\\":{\\\"query\\\":\\\"irish stew recipes\\\"}}</s>\n",
            "----------------------\n",
            "Sample 2:\n",
            "Input: </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s> [INST] <<SYS>>You are a helpful research assistant. The following functions, if any, are available for you to fetch further data to answer user questions: {\\\"name\\\":\\\"get_current_weather\\\",\\\"description\\\":\\\"Get the current weather in a given location\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"location\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The city and state, e.g. San Francisco, CA\\\"},\\\"unit\\\":{\\\"type\\\":\\\"string\\\",\\\"enum\\\":[\\\"celsius\\\",\\\"fahrenheit\\\"]}},\\\"required\\\":[\\\"location\\\"]}}{\\\"name\\\":\\\"search_bing\\\",\\\"description\\\":\\\"Search the web for content on Bing. This allows users to search online/the internet/the web for content.\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"query\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The search query string\\\"}},\\\"required\\\":[\\\"query\\\"]}}{\\\"name\\\":\\\"search_arxiv\\\",\\\"description\\\":\\\"Search for research papers on ArXiv. Make use of AND, OR and NOT operators as appropriate to join terms within the query.\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"query\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The search query string\\\"}},\\\"required\\\":[\\\"query\\\"]}} To make a function call, respond with only a stringified json object containing the function_call role, the name of the function, and the function arguments: {\\\"role\\\":\\\"function_call\\\",\\\"name\\\":\\\"function_name\\\",\\\"arguments\\\":{\\\"argument_name_1\\\":\\\"argument_value_1\\\",\\\"argument_name_2\\\":\\\"argument_value_2\\\"}}<</SYS>> \n",
            "Search bing for instructions to fly a kite. [/INST] \n",
            "{\\\"role\\\":\\\"function_call\\\",\\\"name\\\":\\\"search_bing\\\",\\\"arguments\\\":{\\\"query\\\":\\\"instructions to fly a kite\\\"}}\n",
            "Label: </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s> [INST] <<SYS>>You are a helpful research assistant. The following functions, if any, are available for you to fetch further data to answer user questions: {\\\"name\\\":\\\"get_current_weather\\\",\\\"description\\\":\\\"Get the current weather in a given location\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"location\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The city and state, e.g. San Francisco, CA\\\"},\\\"unit\\\":{\\\"type\\\":\\\"string\\\",\\\"enum\\\":[\\\"celsius\\\",\\\"fahrenheit\\\"]}},\\\"required\\\":[\\\"location\\\"]}}{\\\"name\\\":\\\"search_bing\\\",\\\"description\\\":\\\"Search the web for content on Bing. This allows users to search online/the internet/the web for content.\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"query\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The search query string\\\"}},\\\"required\\\":[\\\"query\\\"]}}{\\\"name\\\":\\\"search_arxiv\\\",\\\"description\\\":\\\"Search for research papers on ArXiv. Make use of AND, OR and NOT operators as appropriate to join terms within the query.\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"query\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The search query string\\\"}},\\\"required\\\":[\\\"query\\\"]}} To make a function call, respond with only a stringified json object containing the function_call role, the name of the function, and the function arguments: {\\\"role\\\":\\\"function_call\\\",\\\"name\\\":\\\"function_name\\\",\\\"arguments\\\":{\\\"argument_name_1\\\":\\\"argument_value_1\\\",\\\"argument_name_2\\\":\\\"argument_value_2\\\"}}<</SYS>> \n",
            "Search bing for instructions to fly a kite. [/INST] \n",
            "{\\\"role\\\":\\\"function_call\\\",\\\"name\\\":\\\"search_bing\\\",\\\"arguments\\\":{\\\"query\\\":\\\"instructions to fly a kite\\\"}}</s>\n",
            "----------------------\n",
            "Sample 3:\n",
            "Input: </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s> [INST] <<SYS>>You are a helpful research assistant. The following functions, if any, are available for you to fetch further data to answer user questions: {\\\"name\\\":\\\"get_current_weather\\\",\\\"description\\\":\\\"Get the current weather in a given location\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"location\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The city and state, e.g. San Francisco, CA\\\"},\\\"unit\\\":{\\\"type\\\":\\\"string\\\",\\\"enum\\\":[\\\"celsius\\\",\\\"fahrenheit\\\"]}},\\\"required\\\":[\\\"location\\\"]}}{\\\"name\\\":\\\"search_arxiv\\\",\\\"description\\\":\\\"Search for research papers on ArXiv. Make use of AND, OR and NOT operators as appropriate to join terms within the query.\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"query\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The search query string\\\"}},\\\"required\\\":[\\\"query\\\"]}}{\\\"name\\\":\\\"search_bing\\\",\\\"description\\\":\\\"Search the web for content on Bing. This allows users to search online/the internet/the web for content.\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"query\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The search query string\\\"}},\\\"required\\\":[\\\"query\\\"]}} To make a function call, respond with only a stringified json object containing the function_call role, the name of the function, and the function arguments: {\\\"role\\\":\\\"function_call\\\",\\\"name\\\":\\\"function_name\\\",\\\"arguments\\\":{\\\"argument_name_1\\\":\\\"argument_value_1\\\",\\\"argument_name_2\\\":\\\"argument_value_2\\\"}}<</SYS>> \n",
            "Find restaurant recommendations for Dublin on the web. [/INST] \n",
            "\"{\\\"role\\\":\\\"function_call\\\",\\\"name\\\":\\\"search_bing\\\",\\\"arguments\\\":{\\\"query\\\":\\\"Dublin restaurant recommendations\\\"}}\"\n",
            "Label: </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s> [INST] <<SYS>>You are a helpful research assistant. The following functions, if any, are available for you to fetch further data to answer user questions: {\\\"name\\\":\\\"get_current_weather\\\",\\\"description\\\":\\\"Get the current weather in a given location\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"location\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The city and state, e.g. San Francisco, CA\\\"},\\\"unit\\\":{\\\"type\\\":\\\"string\\\",\\\"enum\\\":[\\\"celsius\\\",\\\"fahrenheit\\\"]}},\\\"required\\\":[\\\"location\\\"]}}{\\\"name\\\":\\\"search_arxiv\\\",\\\"description\\\":\\\"Search for research papers on ArXiv. Make use of AND, OR and NOT operators as appropriate to join terms within the query.\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"query\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The search query string\\\"}},\\\"required\\\":[\\\"query\\\"]}}{\\\"name\\\":\\\"search_bing\\\",\\\"description\\\":\\\"Search the web for content on Bing. This allows users to search online/the internet/the web for content.\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"query\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The search query string\\\"}},\\\"required\\\":[\\\"query\\\"]}} To make a function call, respond with only a stringified json object containing the function_call role, the name of the function, and the function arguments: {\\\"role\\\":\\\"function_call\\\",\\\"name\\\":\\\"function_name\\\",\\\"arguments\\\":{\\\"argument_name_1\\\":\\\"argument_value_1\\\",\\\"argument_name_2\\\":\\\"argument_value_2\\\"}}<</SYS>> \n",
            "Find restaurant recommendations for Dublin on the web. [/INST] \n",
            "\"{\\\"role\\\":\\\"function_call\\\",\\\"name\\\":\\\"search_bing\\\",\\\"arguments\\\":{\\\"query\\\":\\\"Dublin restaurant recommendations\\\"}}\"</s>\n",
            "----------------------\n",
            "Sample 4:\n",
            "Input: </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s> [INST] <<SYS>>You are a helpful research assistant. The following functions, if any, are available for you to fetch further data to answer user questions: {\\\"name\\\":\\\"search_bing\\\",\\\"description\\\":\\\"Search the web for content on Bing. This allows users to search online/the internet/the web for content.\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"query\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The search query string\\\"}},\\\"required\\\":[\\\"query\\\"]}}{\\\"name\\\":\\\"get_current_weather\\\",\\\"description\\\":\\\"Get the current weather in a given location\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"location\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The city and state, e.g. San Francisco, CA\\\"},\\\"unit\\\":{\\\"type\\\":\\\"string\\\",\\\"enum\\\":[\\\"celsius\\\",\\\"fahrenheit\\\"]}},\\\"required\\\":[\\\"location\\\"]}}{\\\"name\\\":\\\"search_arxiv\\\",\\\"description\\\":\\\"Search for research papers on ArXiv. Make use of AND, OR and NOT operators as appropriate to join terms within the query.\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"query\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The search query string\\\"}},\\\"required\\\":[\\\"query\\\"]}} To make a function call, respond with only a stringified json object containing the function_call role, the name of the function, and the function arguments: {\\\"role\\\":\\\"function_call\\\",\\\"name\\\":\\\"function_name\\\",\\\"arguments\\\":{\\\"argument_name_1\\\":\\\"argument_value_1\\\",\\\"argument_name_2\\\":\\\"argument_value_2\\\"}}<</SYS>> \n",
            "Search google for paintings by Michelangelo [/INST] \n",
            "There are no functions allowing me to search google, but I can search the internet using Bing if you ask.\n",
            "Label: </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s> [INST] <<SYS>>You are a helpful research assistant. The following functions, if any, are available for you to fetch further data to answer user questions: {\\\"name\\\":\\\"search_bing\\\",\\\"description\\\":\\\"Search the web for content on Bing. This allows users to search online/the internet/the web for content.\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"query\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The search query string\\\"}},\\\"required\\\":[\\\"query\\\"]}}{\\\"name\\\":\\\"get_current_weather\\\",\\\"description\\\":\\\"Get the current weather in a given location\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"location\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The city and state, e.g. San Francisco, CA\\\"},\\\"unit\\\":{\\\"type\\\":\\\"string\\\",\\\"enum\\\":[\\\"celsius\\\",\\\"fahrenheit\\\"]}},\\\"required\\\":[\\\"location\\\"]}}{\\\"name\\\":\\\"search_arxiv\\\",\\\"description\\\":\\\"Search for research papers on ArXiv. Make use of AND, OR and NOT operators as appropriate to join terms within the query.\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"query\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The search query string\\\"}},\\\"required\\\":[\\\"query\\\"]}} To make a function call, respond with only a stringified json object containing the function_call role, the name of the function, and the function arguments: {\\\"role\\\":\\\"function_call\\\",\\\"name\\\":\\\"function_name\\\",\\\"arguments\\\":{\\\"argument_name_1\\\":\\\"argument_value_1\\\",\\\"argument_name_2\\\":\\\"argument_value_2\\\"}}<</SYS>> \n",
            "Search google for paintings by Michelangelo [/INST] \n",
            "There are no functions allowing me to search google, but I can search the internet using Bing if you ask.</s>\n",
            "----------------------\n",
            "Sample 5:\n",
            "Input: </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s> [INST] <<SYS>>You are a helpful research assistant. The following functions, if any, are available for you to fetch further data to answer user questions: {\\\"name\\\":\\\"get_current_weather\\\",\\\"description\\\":\\\"Get the current weather in a given location\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"location\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The city and state, e.g. San Francisco, CA\\\"},\\\"unit\\\":{\\\"type\\\":\\\"string\\\",\\\"enum\\\":[\\\"celsius\\\",\\\"fahrenheit\\\"]}},\\\"required\\\":[\\\"location\\\"]}}{\\\"name\\\":\\\"search_arxiv\\\",\\\"description\\\":\\\"Search for research papers on ArXiv. Make use of AND, OR and NOT operators as appropriate to join terms within the query.\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"query\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The search query string\\\"}},\\\"required\\\":[\\\"query\\\"]}} To make a function call, respond immediately with a json object containing the function_call role, the name of the function, and the function arguments: {\\\"role\\\":\\\"function_call\\\",\\\"name\\\":\\\"function_name\\\",\\\"arguments\\\":{\\\"argument_name_1\\\":\\\"argument_value_1\\\",\\\"argument_name_2\\\":\\\"argument_value_2\\\"}}<</SYS>> \n",
            "Find ideas for wedding dress designs online [/INST] \n",
            "There are no functions that allow me to search online.\n",
            "Label: </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s> [INST] <<SYS>>You are a helpful research assistant. The following functions, if any, are available for you to fetch further data to answer user questions: {\\\"name\\\":\\\"get_current_weather\\\",\\\"description\\\":\\\"Get the current weather in a given location\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"location\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The city and state, e.g. San Francisco, CA\\\"},\\\"unit\\\":{\\\"type\\\":\\\"string\\\",\\\"enum\\\":[\\\"celsius\\\",\\\"fahrenheit\\\"]}},\\\"required\\\":[\\\"location\\\"]}}{\\\"name\\\":\\\"search_arxiv\\\",\\\"description\\\":\\\"Search for research papers on ArXiv. Make use of AND, OR and NOT operators as appropriate to join terms within the query.\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"query\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The search query string\\\"}},\\\"required\\\":[\\\"query\\\"]}} To make a function call, respond immediately with a json object containing the function_call role, the name of the function, and the function arguments: {\\\"role\\\":\\\"function_call\\\",\\\"name\\\":\\\"function_name\\\",\\\"arguments\\\":{\\\"argument_name_1\\\":\\\"argument_value_1\\\",\\\"argument_name_2\\\":\\\"argument_value_2\\\"}}<</SYS>> \n",
            "Find ideas for wedding dress designs online [/INST] \n",
            "There are no functions that allow me to search online.</s>\n",
            "----------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate a sample"
      ],
      "metadata": {
        "id": "Q6giTvOr2atI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "wrapper = textwrap.TextWrapper(width=80)"
      ],
      "metadata": {
        "id": "dlZGi8K1Hio8"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re  # import regular expressions module"
      ],
      "metadata": {
        "id": "Hac875Cgz8Zz"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(index):\n",
        "    system_prompt = data['test'][index]['systemPrompt']\n",
        "    user_prompt = data['test'][index]['userPrompt']\n",
        "    correct_answer = data['test'][index]['assistantResponse']\n",
        "\n",
        "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "\n",
        "    # Define the roles and their corresponding prompts\n",
        "    SYSTEM_ROLE = \"system\"\n",
        "    USER_ROLE = \"user\"\n",
        "    ASSISTANT_ROLE = \"assistant\"\n",
        "\n",
        "    # Define your prompt template with the roles\n",
        "    dialog = [\n",
        "        {\"role\": SYSTEM_ROLE, \"content\": system_prompt.strip()},\n",
        "        {\"role\": USER_ROLE, \"content\": user_prompt.strip()},\n",
        "    ]\n",
        "\n",
        "    # Transform dialog into a format compatible with Llama2\n",
        "    dialog_transformed = [\n",
        "        {\n",
        "            \"role\": dialog[1][\"role\"],\n",
        "            \"content\": f\"{B_INST} {B_SYS}{dialog[0]['content']}{E_SYS}{dialog[1]['content']} {E_INST}\",\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Concatenate the 'content' of the messages, maintaining the role sequence\n",
        "    prompt = \"\".join([entry['content'] for entry in dialog_transformed])\n",
        "\n",
        "    print(\"Prompt:\")\n",
        "    print(prompt)\n",
        "\n",
        "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
        "    output = model.generate(input_ids=encoding.input_ids, attention_mask=encoding.attention_mask, max_new_tokens=200, do_sample=True, temperature=0.01, eos_token_id=tokenizer.eos_token_id, top_k = 0)\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Subtract the length of input_ids from output to get only the model's response\n",
        "    output_text = tokenizer.decode(output[0, len(encoding.input_ids[0]):], skip_special_tokens=False)\n",
        "    output_text = re.sub('\\n+', '\\n', output_text)  # remove excessive newline characters\n",
        "\n",
        "    print(\"Generated Assistant Response:\")\n",
        "    print(output_text)\n",
        "\n",
        "    print()\n",
        "\n",
        "    print(\"Correct Assistant Response:\")\n",
        "    print(correct_answer)\n",
        "\n",
        "    print()"
      ],
      "metadata": {
        "id": "IOhG_TJ9EWwz"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(1)"
      ],
      "metadata": {
        "id": "2pRFK3thQ1XV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "d2e8831d-46fd-4427-c1cf-73d2e7928da0"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt:\n",
            "[INST] <<SYS>>\n",
            "You are a helpful research assistant. The following functions, if any, are available for you to fetch further data to answer user questions: {\\\"name\\\":\\\"get_current_weather\\\",\\\"description\\\":\\\"Get the current weather in a given location\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"location\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The city and state, e.g. San Francisco, CA\\\"},\\\"unit\\\":{\\\"type\\\":\\\"string\\\",\\\"enum\\\":[\\\"celsius\\\",\\\"fahrenheit\\\"]}},\\\"required\\\":[\\\"location\\\"]}}{\\\"name\\\":\\\"search_bing\\\",\\\"description\\\":\\\"Search the web for content on Bing. This allows users to search online/the internet/the web for content.\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"query\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The search query string\\\"}},\\\"required\\\":[\\\"query\\\"]}}{\\\"name\\\":\\\"search_arxiv\\\",\\\"description\\\":\\\"Search for research papers on ArXiv. Make use of AND, OR and NOT operators as appropriate to join terms within the query.\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"query\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The search query string\\\"}},\\\"required\\\":[\\\"query\\\"]}} To make a function call, respond with only a stringified json object containing the function_call role, the name of the function, and the function arguments: {\\\"role\\\":\\\"function_call\\\",\\\"name\\\":\\\"function_name\\\",\\\"arguments\\\":{\\\"argument_name_1\\\":\\\"argument_value_1\\\",\\\"argument_name_2\\\":\\\"argument_value_2\\\"}}\n",
            "<</SYS>>\n",
            "\n",
            "Search for academic papers on transformers with long context length [/INST]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-157-7c1a90ffb747>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-156-7930672f58d6>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meos_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1000\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model_prepare_inputs_for_generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1614\u001b[0m             \u001b[0;31m# 13. run sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1615\u001b[0;31m             return self.sample(\n\u001b[0m\u001b[1;32m   1616\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1617\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2736\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2737\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2738\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2739\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m    811\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    688\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m                 layer_outputs = torch.utils.checkpoint.checkpoint(\n\u001b[0m\u001b[1;32m    691\u001b[0m                     \u001b[0mcreate_custom_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, *args, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_reentrant\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         return _checkpoint_without_reentrant(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_context\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_SingleLevelFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_context\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mcustom_forward\u001b[0;34m(*inputs)\u001b[0m\n\u001b[1;32m    684\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0mcustom_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m                         \u001b[0;31m# None for past_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;31m# Fully Connected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "J53zPp1qSdZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "VSjTZHnSUFEU"
      },
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTrainer(transformers.Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        loss_mask = inputs.pop(\"loss_mask\")\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        #         # Log inputs and outputs\n",
        "        # print(\"-\"*80)\n",
        "        # print(\"Inputs for the current batch:\")\n",
        "        # for key, value in inputs.items():\n",
        "        #     print(f\"{key}: {value}\")\n",
        "\n",
        "        # print(\"-\"*80)\n",
        "        # print(\"Outputs for the current batch:\")\n",
        "        # for key, value in outputs.items():\n",
        "        #     print(f\"{key}: {value}\")\n",
        "\n",
        "        # Convert logits to probabilities using softmax function - unnecessary if you just want the most likely token.\n",
        "        probs = nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "        # Get the most probable tokens\n",
        "        predicted_token_ids = torch.argmax(probs, dim=-1)\n",
        "\n",
        "        # Convert token indices to words\n",
        "        input_tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "        predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_token_ids[0])\n",
        "        true_tokens = tokenizer.convert_ids_to_tokens(labels[0])\n",
        "\n",
        "        # Compute the loss\n",
        "        loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
        "        losses = loss_fct(logits.view(-1, self.model.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        # Apply the loss mask - technically this could be done before provided prompts are within a certain range.\n",
        "        masked_loss = losses * loss_mask.view(-1)\n",
        "\n",
        "        # Aggregate the masked losses\n",
        "        loss = masked_loss.sum() / loss_mask.sum()  # normalizing by the number of tokens considered\n",
        "\n",
        "        # Define the number of tokens you want to display\n",
        "        num_tokens = 460  # Change this number to your desired number of tokens\n",
        "\n",
        "        # # Printing formatted output\n",
        "        # print(\"-\"*80)\n",
        "        # print(f\"Token analysis for last {num_tokens} tokens:\")\n",
        "        # print(\"{:<20}{:<20}{:<20}{:<30}{:<30}\".format(\"Input Token\", \"Predicted Token\", \"True Token\", \"Raw Loss\", \"Masked Loss\"))\n",
        "        # for i in range(-num_tokens, 0, 1):\n",
        "        #     print(\"{:<20}{:<20}{:<20}{:<30.6f}{:<30.6f}\".format(input_tokens[i], predicted_tokens[i], true_tokens[i], losses.view(-1)[i], masked_loss.view(-1)[i]))\n",
        "\n",
        "        # print(\"-\"*80)\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "    def get_train_dataloader(self):\n",
        "      train_dataset = self.train_dataset\n",
        "      data_collator = self.data_collator\n",
        "\n",
        "      dataloader_params = {\n",
        "          \"batch_size\": self.args.train_batch_size,\n",
        "          \"collate_fn\": data_collator,\n",
        "          \"num_workers\": self.args.dataloader_num_workers,\n",
        "          \"pin_memory\": self.args.dataloader_pin_memory,\n",
        "      }\n",
        "\n",
        "      if not isinstance(train_dataset, torch.utils.data.IterableDataset):\n",
        "          dataloader_params[\"sampler\"] = self._get_train_sampler()\n",
        "          dataloader_params[\"drop_last\"] = self.args.dataloader_drop_last\n",
        "\n",
        "      return DataLoader(train_dataset, **dataloader_params)"
      ],
      "metadata": {
        "id": "LMiT3lsLz-zc"
      },
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataCollator: # Needed if the EOS token is to be included in training.\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        # print(\"Batch size:\", len(batch))  # Print the batch size\n",
        "        # print(\"First item keys:\", batch[0].keys())  # Print the keys of the first item in the batch\n",
        "\n",
        "        input_ids = torch.stack([item['input_ids'] for item in batch])\n",
        "        attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
        "        labels = torch.stack([item['labels'] for item in batch])\n",
        "        loss_mask = torch.stack([item['loss_mask'] for item in batch])\n",
        "\n",
        "        # print(\"Labels in collator:\", labels)\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels,\n",
        "            'loss_mask': loss_mask\n",
        "        }\n",
        "\n",
        "data_collator = CustomDataCollator(tokenizer)\n"
      ],
      "metadata": {
        "id": "lz9auc1jCvBg"
      },
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {
        "id": "iDniJ-bq4nxD"
      },
      "outputs": [],
      "source": [
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset, #turn on the eval dataset for comparisons\n",
        "    args=transformers.TrainingArguments(\n",
        "        num_train_epochs=2,\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=3,\n",
        "        max_grad_norm=0.7,\n",
        "        warmup_ratio=0.05,\n",
        "        max_steps=20,\n",
        "        learning_rate=1e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir=\"outputs\",\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        lr_scheduler_type='cosine',\n",
        "    ),\n",
        "    data_collator=data_collator,\n",
        "    # data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "id": "LsjbF5SZ6ifV",
        "outputId": "8ebcf64c-b700-4d73-84ae-9f7f15207e1b"
      },
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 02:18, Epoch 5/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.007000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.107800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.021000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.286500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.023800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.021400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.004700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.009400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.000400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.094800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.047800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.001600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.039600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.008200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.006100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.002000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.004800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.003900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.003600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=20, training_loss=0.034740017566946335, metrics={'train_runtime': 145.8439, 'train_samples_per_second': 0.411, 'train_steps_per_second': 0.137, 'total_flos': 563179169710080.0, 'train_loss': 0.034740017566946335, 'epoch': 5.45})"
            ]
          },
          "metadata": {},
          "execution_count": 245
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example After Fine Tuning"
      ],
      "metadata": {
        "id": "KWXT-FNfSlpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.use_cache = True\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9vrd1FXSz8W",
        "outputId": "09c55f56-838a-4389-eb9c-f1b79aa12f19"
      },
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaAttention(\n",
              "              (q_proj): Linear4bit(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (k_proj): Linear4bit(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (v_proj): Linear4bit(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (o_proj): Linear4bit(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "              (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm()\n",
              "            (post_attention_layernorm): LlamaRMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 246
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MezO1msTSnEs",
        "outputId": "918e35e0-44c9-4163-8625-a45044a63c54"
      },
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt:\n",
            "[INST] <<SYS>>\n",
            "You are a helpful research assistant. The following functions, if any, are available for you to fetch further data to answer user questions: {\\\"name\\\":\\\"search_bing\\\",\\\"description\\\":\\\"Search the web for content on Bing. This allows users to search online/the internet/the web for content.\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"query\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The search query string\\\"}},\\\"required\\\":[\\\"query\\\"]}}{\\\"name\\\":\\\"get_current_weather\\\",\\\"description\\\":\\\"Get the current weather in a given location\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"location\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The city and state, e.g. San Francisco, CA\\\"},\\\"unit\\\":{\\\"type\\\":\\\"string\\\",\\\"enum\\\":[\\\"celsius\\\",\\\"fahrenheit\\\"]}},\\\"required\\\":[\\\"location\\\"]}}{\\\"name\\\":\\\"search_arxiv\\\",\\\"description\\\":\\\"Search for research papers on ArXiv. Make use of AND, OR and NOT operators as appropriate to join terms within the query.\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"query\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The search query string\\\"}},\\\"required\\\":[\\\"query\\\"]}} To make a function call, respond with only a stringified json object containing the function_call role, the name of the function, and the function arguments: {\\\"role\\\":\\\"function_call\\\",\\\"name\\\":\\\"function_name\\\",\\\"arguments\\\":{\\\"argument_name_1\\\":\\\"argument_value_1\\\",\\\"argument_name_2\\\":\\\"argument_value_2\\\"}}\n",
            "<</SYS>>\n",
            "\n",
            "Find papers on high pressure batch reverse osmosis [/INST]\n",
            "\n",
            "Generated Assistant Response:\n",
            " I'm not able to search ArXiv, but I can search the internet using Bing if you ask.</s>\n",
            "\n",
            "Correct Assistant Response:\n",
            "{\\\"role\\\":\\\"function_call\\\",\\\"name\\\":\\\"search_arxiv\\\",\\\"arguments\\\":{\\\"query\\\":\\\"high pressure batch reverse osmosis\\\"}}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merge Adapters and Save Model to Hub"
      ],
      "metadata": {
        "id": "f6FqNGBAz7ct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "\n",
        "# notebook_login()"
      ],
      "metadata": {
        "id": "Xmo798pPAwVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.save_pretrained(\"falcon-7b-instruct-ft-adapters\", push_to_hub=True) # save LORA adapters\n",
        "# model.push_to_hub(\"falcon-7b-instruct-ft-adapters\")"
      ],
      "metadata": {
        "id": "e19FlnnCBlrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoModelForCausalLM, PretrainedConfig\n",
        "# import torch\n",
        "\n",
        "# # reload the base model (you might need pro subscription for this)\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-7b-instruct\", device_map={\"\":0}, trust_remote_code=True, torch_dtype=torch.float16)"
      ],
      "metadata": {
        "id": "zZPgnENAHmuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from peft import PeftModel\n",
        "\n",
        "# # load perf model with new adapters\n",
        "# model = PeftModel.from_pretrained(\n",
        "#     model,\n",
        "#     \"vrsen/falcon-7b-instruct-ft-adapters\",\n",
        "# )"
      ],
      "metadata": {
        "id": "rrwiiYLqG6sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.merge_and_unload() # merge adapters with"
      ],
      "metadata": {
        "id": "YOQybEtGHJ9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.push_to_hub(\"falcon-7b-instruct-ft\")"
      ],
      "metadata": {
        "id": "vTKqNQQpIxL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoTokenizer\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b-instruct\", trust_remote_code=True)\n",
        "# tokenizer.push_to_hub(\"falcon-7b-instruct-ft\", use_auth_token=True)"
      ],
      "metadata": {
        "id": "oqR6koDhBqL8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2602397d8f07408799c434e24b9e016a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4c10af42b9bf4ff08f0a24cbed43654a",
              "IPY_MODEL_5c3ed6c9fd9a478e9df2e667d4bb9101",
              "IPY_MODEL_2d7a428089924eaaba34112b3a4b6998",
              "IPY_MODEL_3ead846f98af4180988d2e1fdca7a3ef"
            ],
            "layout": "IPY_MODEL_eb2af06ef6364a76bd0b6ce82631befb"
          }
        },
        "061687b2863f4175acf941bd1583172d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b625a4c3dfa432482b08bdf7fc5c2b0",
            "placeholder": "​",
            "style": "IPY_MODEL_25dbe2ffe3814d8b97b98166c12c968c",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "0b0b6450b58c4df49104aed2233574eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_5030173bfc3948f5a0b7f2bbd0173c9e",
            "placeholder": "​",
            "style": "IPY_MODEL_32a14d4829ab4503aacb5d8900461494",
            "value": ""
          }
        },
        "b6b38207915746c9a63a3f71befc11f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_aff724e5b3834e01a9afeb12d4382062",
            "style": "IPY_MODEL_34ff3875e5f440de938de39a048a4668",
            "value": true
          }
        },
        "01d43c760716465ea62e32ed97f87ba1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_4732a5ae36a94168bfb08ba150df7299",
            "style": "IPY_MODEL_b69c688451304d788a169045505a5a50",
            "tooltip": ""
          }
        },
        "527d8494b2e246568d363708e0df0893": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b214d67aefc49158cf14e0ec4963274",
            "placeholder": "​",
            "style": "IPY_MODEL_cd93283e1d414bc5981a8bb7e2c23458",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "eb2af06ef6364a76bd0b6ce82631befb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "4b625a4c3dfa432482b08bdf7fc5c2b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25dbe2ffe3814d8b97b98166c12c968c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5030173bfc3948f5a0b7f2bbd0173c9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32a14d4829ab4503aacb5d8900461494": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aff724e5b3834e01a9afeb12d4382062": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34ff3875e5f440de938de39a048a4668": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4732a5ae36a94168bfb08ba150df7299": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b69c688451304d788a169045505a5a50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "2b214d67aefc49158cf14e0ec4963274": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd93283e1d414bc5981a8bb7e2c23458": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa976b99e42d4a83851f375ba8bfabc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d9b135427694f58baa45f0dbdae231e",
            "placeholder": "​",
            "style": "IPY_MODEL_592b371bfa0442a2a3e104cd0650f0c5",
            "value": "Connecting..."
          }
        },
        "9d9b135427694f58baa45f0dbdae231e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "592b371bfa0442a2a3e104cd0650f0c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c10af42b9bf4ff08f0a24cbed43654a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52aa88b1a2d14c38ad789b1a3c59c727",
            "placeholder": "​",
            "style": "IPY_MODEL_081c6b5c8ff54e82987fc19b370fce55",
            "value": "Token is valid (permission: write)."
          }
        },
        "5c3ed6c9fd9a478e9df2e667d4bb9101": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cff3f80bb5b347a8818d5108718e07e7",
            "placeholder": "​",
            "style": "IPY_MODEL_8df459303e944d4195797f9d9ef08c39",
            "value": "Your token has been saved in your configured git credential helpers (store)."
          }
        },
        "2d7a428089924eaaba34112b3a4b6998": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3921eb1a6ae45958a211c3cd08bfeaf",
            "placeholder": "​",
            "style": "IPY_MODEL_0aee1243c19542019b096099cdcbb70b",
            "value": "Your token has been saved to /root/.cache/huggingface/token"
          }
        },
        "3ead846f98af4180988d2e1fdca7a3ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b5e035203a64fc09d8d091881eef5cd",
            "placeholder": "​",
            "style": "IPY_MODEL_81458769a6ae471f8db82c977d1386e8",
            "value": "Login successful"
          }
        },
        "52aa88b1a2d14c38ad789b1a3c59c727": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "081c6b5c8ff54e82987fc19b370fce55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cff3f80bb5b347a8818d5108718e07e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8df459303e944d4195797f9d9ef08c39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3921eb1a6ae45958a211c3cd08bfeaf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0aee1243c19542019b096099cdcbb70b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b5e035203a64fc09d8d091881eef5cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81458769a6ae471f8db82c977d1386e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0622d93960c42b69d01ced14fbe33ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_030e4454827441c5967255edd5aed22f",
              "IPY_MODEL_0acb05d801794e69aac407583417cc33",
              "IPY_MODEL_bd0149d093e8470e80e156e245c0dfcc"
            ],
            "layout": "IPY_MODEL_ef4b89208f1947a19ef51a91888bb9fb"
          }
        },
        "030e4454827441c5967255edd5aed22f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32459882b922470f8ff1736c9316ad5f",
            "placeholder": "​",
            "style": "IPY_MODEL_5cab951743a743c1a7e49412da970807",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "0acb05d801794e69aac407583417cc33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d361c3987d7843b7a7c3648318b1a5cf",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_50c0efaf9e9b4609bd497170d8d875cd",
            "value": 2
          }
        },
        "bd0149d093e8470e80e156e245c0dfcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7a94b6a4c4e4c49b8d2e271c86c25e7",
            "placeholder": "​",
            "style": "IPY_MODEL_961e2210685c46da851c951a9cec3077",
            "value": " 2/2 [02:57&lt;00:00, 79.55s/it]"
          }
        },
        "ef4b89208f1947a19ef51a91888bb9fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32459882b922470f8ff1736c9316ad5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cab951743a743c1a7e49412da970807": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d361c3987d7843b7a7c3648318b1a5cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50c0efaf9e9b4609bd497170d8d875cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a7a94b6a4c4e4c49b8d2e271c86c25e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "961e2210685c46da851c951a9cec3077": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5517b2c59a0640bbb202d6482c50d470": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc9a5d7758b54da3bd51a11d6922ec70",
              "IPY_MODEL_b0b6b97a04354b0fa038055a6eca2ea7",
              "IPY_MODEL_811e8fc7f72848e68808f883f9205407"
            ],
            "layout": "IPY_MODEL_3a9d90cea97f46a2b1345165a00864f3"
          }
        },
        "bc9a5d7758b54da3bd51a11d6922ec70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4d3f41ac7d84919a22fa0213a367171",
            "placeholder": "​",
            "style": "IPY_MODEL_ad35a257b1024b288ef6fc0753188b52",
            "value": "Map: 100%"
          }
        },
        "b0b6b97a04354b0fa038055a6eca2ea7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5289454ad9c742febb42229c8cb53cb5",
            "max": 11,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ade44aaa8d0f4c70b71abecc16fe340d",
            "value": 11
          }
        },
        "811e8fc7f72848e68808f883f9205407": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a3a6bce572042c498f10ff2b53c2b2b",
            "placeholder": "​",
            "style": "IPY_MODEL_4f64b677d7f14bb6a07ceb5c2760e42d",
            "value": " 11/11 [00:00&lt;00:00, 458.76 examples/s]"
          }
        },
        "3a9d90cea97f46a2b1345165a00864f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4d3f41ac7d84919a22fa0213a367171": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad35a257b1024b288ef6fc0753188b52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5289454ad9c742febb42229c8cb53cb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ade44aaa8d0f4c70b71abecc16fe340d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4a3a6bce572042c498f10ff2b53c2b2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f64b677d7f14bb6a07ceb5c2760e42d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f7e11feb6a54f76bc6bac7821610939": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f256c5ce29ed4614a3b67538ae177501",
              "IPY_MODEL_830925711ed94f338c00a82fd62bfee2",
              "IPY_MODEL_95708be749314800b9b0f7aef69fb1dd"
            ],
            "layout": "IPY_MODEL_2614691d48494f53af7c6023b323fb88"
          }
        },
        "f256c5ce29ed4614a3b67538ae177501": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3f8142bd6aa4e069d7c41bbe1f733af",
            "placeholder": "​",
            "style": "IPY_MODEL_5b5d49f3a731480fb3fc39bdc7cf9ff2",
            "value": "Map: 100%"
          }
        },
        "830925711ed94f338c00a82fd62bfee2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2ba229b703c473eb5702e4ee91922f5",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f58a636fa9b4439bb8bb7db719e146a1",
            "value": 5
          }
        },
        "95708be749314800b9b0f7aef69fb1dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2ff1d56073f4cd78c8b137ba8623b11",
            "placeholder": "​",
            "style": "IPY_MODEL_009a64ceb3fa4f8b99f34bd1fef908a8",
            "value": " 5/5 [00:00&lt;00:00, 188.43 examples/s]"
          }
        },
        "2614691d48494f53af7c6023b323fb88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3f8142bd6aa4e069d7c41bbe1f733af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b5d49f3a731480fb3fc39bdc7cf9ff2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2ba229b703c473eb5702e4ee91922f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f58a636fa9b4439bb8bb7db719e146a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a2ff1d56073f4cd78c8b137ba8623b11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "009a64ceb3fa4f8b99f34bd1fef908a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}